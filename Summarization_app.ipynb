{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2a22a3",
   "metadata": {},
   "source": [
    "\n",
    "# Summarization Application with LLMs\n",
    "\n",
    "In this notebook, I'll build a summarization application with LLMs \n",
    "I use existing, open-source models. For this, I use [Hugging Face models](https://huggingface.co/models) and   prompt engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23068b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sacremoses==0.0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635900f",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarization can take two forms:\n",
    "* `extractive` (selecting representative excerpts from the text)\n",
    "* `abstractive` (generating novel text summaries)\n",
    "\n",
    "Here, I will use a model which does *abstractive* summarization.\n",
    "\n",
    "**Background reading**: The [Hugging Face summarization task page](https://huggingface.co/docs/transformers/tasks/summarization) lists model architectures which support summarization. The [summarization course chapter](https://huggingface.co/course/chapter7/5) provides a detailed walkthrough.\n",
    "\n",
    "In this section, I will use:\n",
    "* **Data**: [xsum](https://huggingface.co/datasets/xsum) dataset, which provides a set of BBC articles and summaries.\n",
    "* **Model**: [t5-small](https://huggingface.co/t5-small) model, which has 60 million parameters (242MB for PyTorch).  T5 is an encoder-decoder model created by Google which supports several tasks such as summarization, translation, Q&A, and text classification.  For more details, see the [Google blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html), [code on GitHub](https://github.com/google-research/text-to-text-transfer-transformer), or the [research paper](https://arxiv.org/pdf/1910.10683.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We specify cache_dir to use predownloaded data.\n",
    "xsum_dataset = load_dataset(\n",
    "    \"xsum\", \n",
    "    version=\"1.2.0\",\n",
    "    cache_dir=DA.paths.datasets,\n",
    "    #verification_mode=\"no_checks\"\n",
    ")\n",
    "\n",
    "xsum_dataset  # The printed representation of this object shows the `num_rows` of each dataset split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55650a3e",
   "metadata": {},
   "source": [
    "This dataset provides 3 columns:\n",
    "* `document`: the BBC article text\n",
    "* `summary`: a \"ground-truth\" summary --> Note how subjective this \"ground-truth\" is.  Is this the same summary you would write?  This a great example of how many LLM applications do not have obvious \"right\" answers.\n",
    "* `id`: article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_sample = xsum_dataset[\"train\"].select(range(10))\n",
    "display(xsum_sample.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2501b4",
   "metadata": {},
   "source": [
    "I next use the Hugging Face `pipeline` tool to load a pre-trained model.  In this LLM pipeline constructor, I specify:\n",
    "* `task`: This first argument specifies the primary task.  See [Hugging Face tasks](https://huggingface.co/tasks) for more information.\n",
    "* `model`: This is the name of the pre-trained model from the [Hugging Face Hub](https://huggingface.co/models).\n",
    "* `min_length`, `max_length`: I want our generated summaries to be between these two token lengths.\n",
    "* `truncation`: Some input articles may be too long for the LLM to process.  Most LLMs have fixed limits on the length of input sequences.  This option tells the pipeline to truncate the input if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ada399",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"t5-small\", # google/pegasus-xsum    t5-small\n",
    "    min_length=20,\n",
    "    max_length=40,\n",
    "    truncation=True,\n",
    "    model_kwargs={\"cache_dir\": DA.paths.datasets},\n",
    ")  # Note: We specify cache_dir to use predownloaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to 1 article\n",
    "summarizer(xsum_sample[\"document\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to a batch of articles\n",
    "results = summarizer(xsum_sample[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cddcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated summary side-by-side with the reference summary and original document.\n",
    "# We use Pandas to join the inputs and outputs together in a nice format.\n",
    "import pandas as pd\n",
    "\n",
    "display(\n",
    "    pd.DataFrame.from_dict(results)\n",
    "    .rename({\"summary_text\": \"generated_summary\"}, axis=1)\n",
    "    .join(pd.DataFrame.from_dict(xsum_sample))[\n",
    "        [\"generated_summary\", \"summary\", \"document\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383e2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
